# -*- mode:org; -*-

#+title:Build Apps & Websites with Firebase
#+subtitle:{{{version}}} {{{date}}}
#+author:LOLH
#+date:2021-11-13 07:12
#+macro:version Version 0.2.0
#+macro:upload-date (eval (current-time-string))
#+bucket:pinecone-forest.com

{{{version}}} {{{date}}}

#+texinfo:@insertcopying


* Deploy a Hugo Website with Cloud Build and Firebase Pipeline
:PROPERTIES:
:CUSTOM_ID: GSP747
:INSTRUCTOR: Google Cloud Training
:END:
- https://googlecoursera.qwiklabs.com/focuses/18470069?parent=lti_session

In this lab you will:

- create a  pipeline for  deploying websites  based on  Hugo, a  static website
  builder.

- You will store the website content in Cloud Source Repositories and

- deploy the website with Firebase,

- then use Cloud Build to create a pipeline to automatically deploy new content
  that is committed to the repository.

** About the Course---Overview
*** Objectives and Skills
In this lab you will learn the following:

- An overview of static websites

- Setting up a website with Hugo

- Storing the website content in Cloud Source Repositories

- Deploying the website with Firebase

- Creating a build pipeline with Cloud Build to automate the deployment


The skills you will gain include:

- Deploying Hugo Websites
- Serverless Applications
- Firebase

*** Prerequisites
The  instructions  provided here  are  sufficient  to  guide you  through  this
lab. You  may also find  it helpful to have  some hands-on experience  with the
services you will be using.

Here are some other labs you may find helpful:

- [[#GSP121][Cloud Source Repositories: Qwik Start]]
- [[#GSP644][Build a Serverless App with Cloud Run that Creates PDF Files]]
- [[#GSP065][Firebase Web]]

*** Self-Paced Lab
Welcome to  *Deploy a Hugo Website  with Cloud Build and  Firebase Pipeline*, a
lab-based  project. Before  diving into  the  lab, please  take a  look at  the
objectives and structure.

**** Objectives
A  Self-Paced  Lab is  an  online  learning environment  along  with  a set  of
instructions to  walk you  through a live,  real-world, and  scenario-based use
case.  In a  lab, you have access  to the actual environment you  want to learn
about, not a simulation or demo environment. You can access the lab environment
from anywhere on the Internet using a standard browser.

**** Project Structure
This lab-based project is divided into 2 main parts:

1. Lab Overview: This introductory reading item.

2.  *Self-Paced Lab:  Deploy  a  Hugo Website  with  Cloud  Build and  Firebase
   Pipeline*. This  is the hands-on  lab that we will  work on in  the Qwiklabs
   platform.

**** Lab Structure
Each  lab has  a  unique set  of  activities  and goals.  Please  refer to  the
instructions and tips in the lab to ensure you're able to successfully complete
it.

**** About Google Cloud Self-Paced Labs
This  Self-Paced   Lab  runs  on   Google  Cloud's  hands-on   platform  called
Qwiklabs.  On  Qwiklabs,  you  do  projects   in  a  hands-on  manner  in  your
browser. You will  get instant access to pre-configured  cloud environment that
has everything you  need to successfully complete this Self-Paced  Lab. So, you
can just focus on the learning.

*Note*: you will have timed access to  the online environment. You will need to
complete the  lab within  the allotted  time, though  you will  have additional
attempts. Please download any files you would like to save prior to your access
expiring.

If you  need help troubleshooting  in the Lab  or are experiencing  issues with
embedded Qwiklabs on Coursera, please refer  to our [[https://support.google.com/qwiklabs/?hl=en#topic=9114857][Help Center]] as a self-serve
option, or contact support@qwiklabs.com.

**** Earn a Coursera Certificate
After  you have  completed the  *Deploy  a Hugo  Website with  Cloud Build  and
Firebase Pipeline* lab-based project you will  be able to earn your certificate
for this lab-based  project. There are additional resources  also available for
you to continue your learning journey!

*** The Benefits of Static Websites

#+cindex:static site builders
*Static site builders* like _Hugo_ have become popular because of their ability
to produce websites that /do not require web servers/.

- With static web platforms there are  no server operating systems or software to
  maintain.

- There are, however, various operational considerations.

  - For example, you  may want to version control your  postings,
    #+cindex:content delivery network (CDN)
    #+cindex:CDN, content delivery network
  - host your web site on a *content delivery network* ("CDN") and
    #+cindex:ssl certificate
  - provision an SSL certificate.

    #+cindex:continuous integration
    #+cindex:continuous deployment
    #+cindex:pipeline
You can  address these  needs by  using a  *Continuous Integration  /
Continuous Deployment pipeline* on  Google Cloud.

- A deployment  pipeline enables developers  to rapidly innovate  by automating
  the  entire deployment  process.

- In  this lab,  you will  learn  to build  a pipeline  that demonstrates  this
  automation.

** The Course
*** Setup
Read these instructions.  Labs are timed and you cannot pause them.  The timer,
which starts when you click =Start  Lab=, shows how long Google Cloud resources
will be made available to you.

This hands-on  lab lets  you do  the lab  activities yourself  in a  real cloud
environment, not in a simulation or demo environment.  It does so by giving you
new, temporary credentials that you use to  sign in and access Google Cloud for
the duration of the lab.

**** What You Need
To complete this lab, you need:

- Access to a standard internet browser (Chrome browser recommended).
- Time to complete the lab.

*Note:* If you already have your  own personal Google Cloud account or project,
do not use it for this lab.

*Note:* If you  are using a Chrome  OS device, open an Incognito  window to run
this lab.

**** How to start your lab and sign in to the Google Cloud Console

#+cindex:credentials, temporary
1. Click  the =Start Lab=  button.  If you  need to pay  for the lab,  a pop-up
   opens  for you  to select  your  payment method.   On  the left  is a  panel
   populated with the *temporary credentials* that you must use for this lab.

   [[../resources/images/-tHp4GI5VSDyTtdqi3qDFtevuY014F88+Fow-adnRgE=.png]]

2. Copy the =username=, and then click =Open Google Console=.  The lab spins up
   resources, and then opens another tab that shows the =Sign in= page.

   [[../resources/images/VkUIAFY2xX3zoHgmWqYKccRLwFrR4BfARLd5ojmlbhs=.png]]

   *Tip:* Open the tabs in separate windows, side-by-side.

   If you see the =Choose an account= page, click =Use Another Account=.

   [[../resources/images/eQ6xPnPn13GjiJP3RWlHWwiMjhooHxTNvzfg1AL2WPw=.png]]

3.  In the *Sign  in* page, paste the =username= that you  copied from the left
   panel. Then copy and paste the =password=.

   *Important:* You must use the credentials  from the left panel.  Do not use
   your Google Cloud  Training credentials.  If you have your  own Google Cloud
   account, do not use it for this lab (avoids incurring charges).

4. Click through the subsequent pages:

   - Accept the terms and conditions.
   - Do not add recovery options  or two-factor authentication (because this is
     a temporary account).
   - Do not sign up for free trials.

   After a few moments, the Cloud Console opens in this tab.

   *Note:* You  can view  the menu with  a list of  Google Cloud  Products and
   Services by clicking the Navigation menu at the top-left.

   [[../resources/images/9vT7xPlxoNP-PsK0J8j0ZPFB4HnnpaIJVCDByaBrSHg=.png]]

*** Process Overview
Here's a diagram of what you are going to build:

[[../resources/images/j0UawtzBJczZe32K8wKYpiVTkIQyKSFMBfyALUzMn+I=.png]]

- Create Hugo code for website
- Commit Hugo code to Cloud Source Repositories, trigger Cloud Build
- Download Hugo; download Firebase
- Install Hugo; install Firebase
- Build website with Hugo, deploy website with Firebase
- Website

#+cindex:pipeline
#+cindex:deploy website
#+cindex:website, deploy
#+cindex:manual deploy
#+cindex:deploy manually
The goal is  to be able to commit  code and have it trigger  the pipeline which
will in turn deploy the website.

#+texinfo:@heading Your  journey will be divided into two parts.

1.  First, you  will *build  the  website locally*  and deploy  it to  Firebase
   manually so you can gain an understanding of the entire process.

2. Second,  you will *automate the  process* by building a  pipeline with Cloud
   Build.

*** Manual Deployment---Build the Website Locally
#+cindex:Linux instance
#+cindex:Firebase initialization
#+cindex:website, local
#+cindex:deploy manually
#+cindex:manual deployment
*First*  build  the  website  manually  *on a  Linux  instance*  to  learn  the
end-to-end process.   You will also use  the Linux instance to  perform some of
the one-time tasks that are needed to get Firebase up and running.

**** Connect to the Linux instance
#+cindex:Linux instance, connect
#+cindex:instance, VM
#+cindex:VM instance
#+cindex:IP address, VM instance
#+cindex:external IP address
#+cindex:SSH
1. From  the navigation menu select  *Compute Engine > VM  Instances*. You will
   see one instance that has been built for you.

2. At  the end of  the line you  should see an External  IP address and  an SSH
   button as shown in the figure below. If these are obscured by an information
   panel, close that panel so you can see the entire line.

   [[file:../resources/images/m+Kk3gckmiT-sXRorACHbhxmjyMqFcSM8+2KmaBvPVs=.png]]

3. Make a note of the External IP address for later use.

4. Click =SSH=.  A window will appear and you will see a shell prompt.

**** Install Hugo locally
#+cindex:Hugo, install on local
#+cindex:local Hugo installation
#+cindex:install Hugo
#+cindex:Linux instance
#+cindex:shell script
Now install Hugo locally in the Linux instance so that you can test the website
locally before deploying it with Firebase.  A shell script has been provided to
make this easier.

1. In  the Linux instance shell,  examine the file ~installhugo.sh~  by running
   ~cat /tmp/installhugo.sh~. You can also see the contents below:

#+pindex:installhugo.sh
#+caption: Install Hugu Shell Script
#+name: installhugo.sh
#+begin_src sh
  #!/bin/bash
  # Copyright 2020 Google Inc. All rights reserved.
  #
  # Licensed under the Apache License, Version 2.0 (the "License");
  # you may not use this file except in compliance with the License.
  # You may obtain a copy of the License at
  #
  #     http://www.apache.org/licenses/LICENSE-2.0
  #
  # Unless required by applicable law or agreed to in writing, software
  # distributed under the License is distributed on an "AS IS" BASIS,
  # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  # See the License for the specific language governing permissions and
  # limitations under the License.
  _HUGO_VERSION=0.69.2
  echo Downloading Hugo version $_HUGO_VERSION...
  wget \
    --quiet \
    -O hugo.tar.gz \
    https://github.com/gohugoio/hugo/releases/download/v${_HUGO_VERSION}/hugo_extended_${_HUGO_VERSION}_Linux-64bit.tar.gz
  echo Extracting Hugo files into /tmp...
  mv hugo.tar.gz /tmp
  tar -C /tmp -xzf /tmp/hugo.tar.gz
  echo The Hugo binary is now at /tmp/hugo.
#+end_src

2. [@2]Note  the use  of the  ~wget~ command  to download  Hugo and  the ~tar~
  command to unpack the Hugo archive.   You will see similar commands later in
  this lab when you create the pipeline.

3. Enter the commands below to run the script and install Hugo:

: cd ~
: /tmp/installhugo.sh

You will receive a message saying that  Hugo has been installed into the ~/tmp~
directory as shown below.  You are ready to build the website infrastructure.

[[file:../resources/images/buO1W3FIuVRjQaEsyrepa9J+JqixRptP1GuVp+GQ-wg=.png]]

**** Create a repository and the initial web site

#+cindex:Cloud Source Repository
#+cindex:repository, create
#+cindex:web site
#+cindex:Linux instance
#+cindex:clone repository
#+cindex:gcloud command
Now
 - *create* a *Cloud Source Repository* to  hold the web site and then
 - *clone* the repository to the Linux instance.
/Cloning/ a repository creates a mirror of it in the shell.  This allows you to
implement the web site while in the  shell and later commit your changes to the
file system.  Later in this lab, you  will set up a *pipeline* that responds to
these commits to the repository.

1. Enter the following commands in the Linux instance shell:

#+begin_src sh
cd ~
gcloud source repos create my_hugo_site
gcloud source repos clone my_hugo_site
#+end_src

You will receive  confirmations about the /creation of the  repository/ and the
/cloning of the repository/  as shown in the figure below.   You can ignore the
two  warning  messages  about  the  charge for  the  repository  and  that  the
repository is empty.

[[file:../resources/images/6Xp6TnJjneDe6pW6IjtEYYz5+Fh8GZ-ezG8mJC37CUw=.png]]

Click =Check my progress= to verify the objective.
#+cindex:hugo new site
2. [@2]Now  you are ready to  *create the site structure*.   Enter the commands
   below in the Linux shell.

: cd ~
: /tmp/hugo new site my_hugo_site --force

Normally the ~hugo~ command creates the directory.  The ~--force~ option will
create the site in the repository directory, which already exists.  This allows
you to keep the Git-related information in the directory that you just cloned.
You will see messages indicating that the site has been created as shown in the
figure below.

[[file:../resources/images/44IDjiRjRFJhaNI9hpuTrTEUj40jAY4h6rpe5d9JUy8=.png]]

#+cindex:theme, Ananke
#+cindex:Ananke theme
#+cindex:hugo theme
#+cindex:submodule, git
#+cindex:Linux instance
3. [@3]Now install the *Ananke theme* to provide a layout for your site.  Enter
   the following commands in the Linux instance shell:

   #+begin_src sh
   cd ~/my_hugo_site
   git submodule add \
     https://github.com/budparr/gohugo-theme-ananke.git \
     themes/ananke
   echo 'theme = "ananke"' >> config.toml
   #+end_src

   You will  see messages indicating that  the theme has been  cloned, as shown
   below.

   [[file:../resources/images/iYg2p-0+z4Vfpl2aqdojAlxQHJx1c3nVMsdHczChPVo=.png]]

   #+cindex:TCP port
   #+cindex:port, TCP
   #+cindex:web site, preview
4. [@4]With  the structure of the  web site set  up, you can now  *preview* it.
   Enter the command below to launch the site at TCP port 8080:

   : cd ~/my_hugo_site
   : /tmp/hugo server -D --bind 0.0.0.0 --port 8080

   #+cindex:build web site
   #+cindex:web site, build
   Hugo will build the  site and serve it for access on TCP  port 8080 as shown
   in the figure  below.  The server will  run until it is  stopped by pressing
   =Ctrl+C=.

   [[file:../resources/images/1A3uN+W8FrMEnQ7S-VX-ZroU9czLv1Z7AV+-3HS71ME=.png]]

   #+cindex:external IP address
   #+cindex:IP address, external
5.  [@5]Open a browser tab and browse  to the external IP address at port 8080.
   Use the following URL, replacing [EXTERNAL  IP] with the external IP address
   of your instance:

   : http://[EXTERNAL IP]:8080

   #+cindex:web site, look like
   The web site should look like this.

   [[file:../resources/images/iGCS5ksMweWQdrBe9CzcnANJg42pSIiaABQsbNK-MW4=.png]]

   Click =Check my progress= to verify the objective.

6. [@6]Go back to the Linux shell and press =Ctrl+C= to stop the Hugo server.

**** Add Firebase to your project
#+cindex:Firebase
#+cindex:deploy to Firebase
#+cindex:website, deploy to Firebase
Now  that you  know what  the website  looks like,  it's time  to deploy  it to
Firebase.
#+texinfo: @heading First enable Firebase within your existing project.

#+cindex:Firebase console
#+cindex:Firebase, enable
#+cindex:project, Firebase
1. Open  a new  tab in your  browser then  open this  [[https://console.firebase.google.com/][link]] in it  to go  to the
   Firebase console.[fn:1]

   Now  click =Add  project=.  You  will be  asked to  select a  name for  your
   project.  Click  inside of the  name field  and select your  existing Google
   Cloud project  that starts  with "qwiklabs-gcp-..." as  shown in  the figure
   below:

   [[file:../resources/images/68ovTX3M+k+Uyqq4+dLY2ugDUsW1Lumcg2buOFhUFWs=.png]]

2. Accept the Firebase terms, then click =Continue=.

3. You may be  asked to confirm the Firebase billing  plan.  The Firebase costs
   are included with the lab. If you are prompted, click =Confirm plan=.

4. You will be  asked to acknowledge some of the  criteria when adding Firebase
   to a project. Click =Continue=.

5. You will be  asked to confirm the use of Google  Analytics for this Firebase
   project.  Since this is a lab  environment, use the toggle to disable Google
   Analytics  and click  Add  Firebase.   It will  take  about  one minute  for
   Firebase to be added to the project.

6. Click =Continue= if prompted after Firebase is added.

**** Deploy the site to Firebase

#+cindex:Firebase CLI
#+cindex:CLI, Firebase
#+cindex:initialize Firebase
#+cindex:Firebase, initialize
#+cindex:@command{firebase init}
#+cindex:@command{firebase deploy}
#+cindex:hosting URL
#+cindex:Hosting, Firebase option
#+cindex:URL, hosting
#+cindex:Firebase Hosting option
1. Install *Firebase CLI* in the Linux instance shell:

   : curl -sL https://firebase.tools | bash

2. Now you need to /initialize/ Firebase. Enter the command below into the shell:

   : cd ~/my_hugo_site
   : firebase init

3. Select *Hosting*  using the  arrow keys  and spacebar.
   - When asked for a /project option/,  select =Use an existing project=, then
     use the arrow  keys, spacebar, and the =Enter= key  to select the *Project
     ID*  provided on  the lab  instruction  page.

   - For the /public directory/, select the default value ~public~.

   - For configuring as  a /single page application/, select  the default value
     of =N=.

   - For setting up /automatic builds and deploys with GitHub/, select =N=.

   - If asked to overwrite any existing files, select =Y=.

4. You  are ready to /deploy/  the application.  Enter the  commands below into
   the Linux instance shell to rebuild the site with Hugo and to deploy it with
   Firebase:

   : /tmp/hugo && firebase deploy

5. After the  application has been deployed, you will  receive a *hosting URL*.
   Click  on it  and  you will  see  the  same website  being  served from  the
   *Firebase  CDN* (/content  delivery  network/).  If  you  receive a  generic
   "welcome" message,  wait a  few minutes  for the CDN  to be  initialized and
   refresh the browser window.  Save this hosting URL for later use.

You  have now  performed  the  entire deployment  locally.  Next, automate  the
process from end to end using Cloud Build.

*** Automate the Deployment
**** Perform the Initial Commit
#+cindex:trigger builds
#+cindex:pipeline
#+cindex:repository changes, trigger builds
#+cindex:@command{git} global parameters
The goal of building the pipeline is  to be able to trigger builds when changes
are made to the repository.  You will  start by performing an initial commit to
the repository so that you can validate your ability to make future changes.

1. Configure  the ~git~  command's global parameters  by entering  the commands
   below into the Linux  shell.  Use your name (or any name  you wish) in place
   of =GIT_NAME=. Use the =username/e-mail= address you were given for this lab
   for the =GIT_EMAIL= value.  Make sure to include the quotation marks.

   : git config --global user.name "[GIT_NAME]"
   : git config --global user.email "[GIT_EMAIL]"

2. Enter the commands below in the Linux shell to create a ~.gitignore~ file to
   exclude certain directories from the repository:

   : cd ~/my_hugo_site
   : echo "resources" >> .gitignore

3. Perform the initial commit to the repository by entering the commands below:

   : git add .
   : git commit -m "Add app to Cloud Source Repositories"
   : git push -u origin master

   You have  now committed  (uploaded) the  initial version  of the  website to
   Google Cloud.

**** Configure the Build
#+findex:cloudbuild.yaml
#+cindex:Cloud Build configuration
#+cindex:Linux instance
*Cloud Build* uses a file named ~cloudbuild.yaml~ /in the root directory of the
repository/ to perform  the build.  The file is in  =YAML= format.  Spacing and
indentation are important, so it has  already been placed on the Linux instance
for you.

1. Enter the command below in the  Linux shell.  Note the final period (".") at
   the end of the ~cp~ command:

   : cd ~/my_hugo_site
   : cp /tmp/cloudbuild.yaml .

2. Run the following to see what the ~cloudbuild.yaml~ file looks like.  Some
   of the lines have wrapped because of their length.

   : cat cloudbuild.yaml

   #+caption: Cloudbuild YAML
   #+name: cloudbuild.yaml
   #+begin_src sh
     # Copyright 2020 Google Inc. All rights reserved.
     #
     # Licensed under the Apache License, Version 2.0 (the "License");
     # you may not use this file except in compliance with the License.
     # You may obtain a copy of the License at
     #
     #     http://www.apache.org/licenses/LICENSE-2.0
     #
     # Unless required by applicable law or agreed to in writing, software
     # distributed under the License is distributed on an "AS IS" BASIS,
     # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     # See the License for the specific language governing permissions and
     # limitations under the License.
     steps:
     - name: 'gcr.io/cloud-builders/wget'
       args:
       - '--quiet'
       - '-O'
     # Unless required by applicable law or agreed to in writing, software
     # distributed under the License is distributed on an "AS IS" BASIS,
     # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     # See the License for the specific language governing permissions and
     # limitations under the License.
     steps:
     - name: 'gcr.io/cloud-builders/wget'
       args:
       - '--quiet'
       - '-O'
       - 'firebase'
       - 'https://firebase.tools/bin/linux/latest'
     - name: 'gcr.io/cloud-builders/wget'
       args:
       - '--quiet'
       - '-O'
       - 'hugo.tar.gz'
       - 'https://github.com/gohugoio/hugo/releases/download/v${_HUGO_VERSION}/hugo_extended_${_HUGO_VERSION}_Linux-64bit.tar.gz'
       waitFor: ['-']
     - name: 'ubuntu:18.04'
       args:
       - 'bash'
       - '-c'
       - |
	 mv hugo.tar.gz /tmp
	 tar -C /tmp -xzf /tmp/hugo.tar.gz
	 mv firebase /tmp
	 chmod 755 /tmp/firebase
	 /tmp/hugo
	 /tmp/firebase deploy --project ${PROJECT_ID} --non-interactive --only hosting -m "Build ${BUILD_ID}"
     substitutions:
       _HUGO_VERSION: 0.69.2
   #+end_src

3. [@3]Here are some observations about the ~cloudbuild.yaml~ file:

   - There are three *named steps* in this file each of which is performed by a
     container image.  The first two  steps use a *Google-supported builder* to
     use ~wget~ to *download the Hugo and Firebase tools*.  These two steps run
     in parallel.   Using the ~wget~  builder is faster than  installing ~wget~
     manually.

   - The third  step uses  a standard  Ubuntu container  to *install  Hugo* and
     *Firebase* after  which the site  is built and deployed.   Installing Hugo
     and Firebase for each deployment allows  you to change the version of Hugo
     whenever you desire while also using the latest version of Firebase.

   - The ~tar~ and  ~wget~ commands are nearly identical to  those used earlier
     in the ~installhugo.sh~ script.

   - The file also uses a  custom substitution variable (=_HUGO_VERSION=) and a
     Google-provided  substitution variable  (=PROJECT_ID=) to  allow for  this
     template to be used in different environments.

   - The Hugo  and Firebase binaries are  created and installed in  a temporary
     directory so  that they do not  inadvertently get deployed to  the website
     itself.

**** Create the Cloud Build Trigger
#+cindex:trigger, create
Now create a *trigger*  that will /respond to commits to  the master branch/ of
the repository.

1. In  the  Cloud Console,  navigate  to  =Navigation  Menu  > Cloud  Build  >
  Triggers=.

2. Click *CREATE TRIGGER*.

3. For the trigger configuration, enter the following details:

| Find                                    | Value                                          |
|-----------------------------------------+------------------------------------------------|
| Name                                    | commit-to-master-branch                        |
| Descrption                              | Push to master                                 |
| Event                                   | Push to a branch                               |
| Repository                              | =my_hugo__site=                                |
| Branch (regex)                          | ~^master$~ (be sure Invert Regex is unchecked) |
| Build Configuration                     | Cloud Build configuration file (yaml or json)  |
| Cloud Build Configuration file location | / cloudbuild.yaml                              |
|-----------------------------------------+------------------------------------------------|

4. [@4]Click *Create*.

**** Update the Cloud Build service account
#+cindex:permissions
#+cindex:IAM
#+cindex:Cloud Console
#+cindex:Cloud Build Service Account
#+cindex:role
The Cloud  Build Service Account needs  to have permissions to  use Firebase to
deploy the website.

1. From the Cloud Console *Navigation menu*, select *IAM & Admin > IAM*.

2.  Locate  the entry  containing ~cloudbuild.gserviceaccount.com~.   Note that
   there is  another service account  that contains cloudbuild.  Make  sure you
   pick the service account  ~cloudbuild.gserviceaccount.com~.  Click the *Edit
   principal* icon,  then click =ADD ANOTHER  ROLE= and add the  role *Firebase
   Products > Firebase Hosting Admin* to it. Click =SAVE=.

**** Test the Pipeline
#+cindex:pipeline, test
#+cindex:trigger build, test
#+cindex:Cloud Build console
#+cindex:console, Cloud Build
#+cindex:Linux instance
#+findex:config.toml
#+cindex:build history
#+cindex:SSL certificate
#+cindex:certificate, SSL
Now that you have created the pipeline, you  can make a change to the site then
commit it to see if the change propagates.

1. In the  Linux shell  enter  the command  below  to move  to the  repository
  directory:

  : cd ~/my_hugo_site

2. Edit  the file ~config.toml~  and change  the title to  something different,
   such as /My Cool New Hugo Site/ and save the changed file.

3. In the  Linux shell, enter the  commands below to commit the  changes to the
   repository and *trigger the Cloud Build pipeline*:

   : git add .
   : git commit -m "I updated the site title"
   : git push -u origin master

4. Go to the Cloud Build console and check the build history.  You should see a
   successful deployment  as shown in  the figure  below.  If not,  consult the
   build details  to identify the problem.   Browse to the hosting  URL you had
   received before.  If you do not have  it, you can go to the Firebase console
   and examine the project to find the  domain name.  It may take a few minutes
   for the  CDN to update.  Note  that the site  has an SSL certificate  and is
   accessed using the https (Hypertext Transfer Protocol Secure) protocol.

   [[file:../resources/images/TIZCeCtiaoYdtnzGKrYKc3C5BDpYrQYY+xAF+NuXgCE=.png]]

   Click /Check my progress/ to verify the objective.

** Congratulations
You have learned  how Cloud Build can orchestrate a  pipeline to quickly deploy
Hugo websites  to Firebase, which  provides a  CDN and SSL  certificate.  Cloud
Build allows  you to  tailor the  process to  adapt to  your needs.   The short
deployment times allow you to innovate  quickly and test your website revisions
with little  effort.  Consult  the Cloud Build  and Firebase  documentation for
more information.

*** Certificate

[[../resources/images/Coursera ABC9H6AUVFT9.pdf]]

*** Finish the Quest
This self-paced lab is part of the [[https://google.qwiklabs.com/quests/148][Qwiklabs Build Apps & Websites with Firebase]]
Quest.   A /Quest/  is a  series of  related labs  that form  a learning  path.
Completing this Quest earns you the badge above, to recognize your achievement.
You can  make your badge  (or badges)  public and link  to them in  your online
resume  or  social media  account.  Enroll  in  this  Quest and  get  immediate
completion  credit if  you've taken  this  lab.  See  other available  Qwiklabs
Quests.

*** Take the Next Lab
Continue your quest with [[https://google.qwiklabs.com/catalog_lab/1464][Google Assistant: Build an Application with Dialogflow
and Cloud Functions]], or try one of these:

- [[https://google.qwiklabs.com/catalog_lab/2163][Importing Data to a Firestore Database]]
- [[https://google.qwiklabs.com/catalog_lab/2166][Build a Serverless Web App with Firebase]]

* Google Assistant---Build an Application with DialogFlow
- https://www.cloudskillsboost.google/focuses/3634?parent=catalog

A Google Cloud Self-Paced Lab---GSP174

** Overview
*Google Assistant* is a personal voice  assistant that offers a host of actions
and integrations. From  making appointments and setting  reminders, to ordering
coffee and playing music, the 1 million+ actions available suit a wide range of
voice command tasks. Google Assistant is offered on Android and iOS, but it can
even  be integrated  with other  devices like  smartwatches, Google  Homes, and
Android TVs.

*Actions*   is   the  central   platform   for   developing  Google   Assistant
applications. The  Actions platform integrates with  human-computer interaction
suites, which simplifies  conversational app development. The  most widely used
suite is Dialogflow, which uses an underlying machine learning (ML) and natural
language understanding (NLU)  schema to build rich  Assistant applications. The
Actions  platform also  integrates with  Cloud  Functions, which  lets you  run
backend  fulfillment  code  in  response  to  events  triggered  by  Dialogflow
requests.

In this  lab, you  will get  hands-on practice with  the Actions  platform, the
Dialogflow  suite,  and  Cloud  Functions  by building  a  "Silly  Name  Maker"
application, which returns a user with a  silly name after they have entered in
a  lucky number  and favorite  color. You  will build  a Dialogflow  agent that
intelligently parses  user input  for specific information.  The agent  will be
supplemented with a  webhook, which will trigger a Cloud  Function that handles
fulfillment logic and returns your user with their silly name.

#+texinfo:@heading What You Will Learn
In this lab, you will learn how to:

- Create an Actions project and build an Action.

- Create a Dialogflow agent and configure the default welcome intent.

- Build a custom intent with entities.

- Initialize a Cloud Function.

- Add fulfillment logic and packages to your Cloud Function.

- Add a webhook to your Action.

- Test your  Assistant application with  the Actions simulator on  expected and
  unexpected conversational paths.

- Optional: test your Assistant application on a Google Home device.

#+texinfo:@heading Prerequisites
This is a *fundamental level lab*. Familiarity with the Actions Console and the
Qwiklabs platform is  expected. If you need  to get up to speed  with the lab's
requirements, please complete one of the following Qwiklabs:

- A Tour of Google Cloud Hands-on Labs

- Google Assistant: Qwik Start - Dialogflow

Since this lab works with the Actions simulator, having a pair of headphones or
turning the volume up on your computer is recommended. If you want to test your
Assistant application on a Google Home, keep your device handy.

Once you're ready,  scroll down and follow  the steps below to set  up your lab
environment.

** Setup
#+texinfo:@heading Cloud Console
#+texinfo:@subheading How to start your lab and sign in to the Google Cloud Console

1. Click the *Start Lab* button. If you need to pay for the lab, a pop-up opens
   for you to select your payment method. On the left is a panel populated with
   the temporary credentials that you must use for this lab.

2. Copy the username,  and then click *Open Google Console*.   The lab spins up
   resources, and then opens another tab that shows the *Sign in* page.

   *Tip:* Open the tabs in separate windows, side-by-side.

3.  In  the *Sign in* page,  paste the username  that you copied from  the left
   panel. Then copy and paste the password.

   *Important:* You  must use the credentials  from the left panel.  Do not use
   your Google  Cloud Training credentials. If  you have your own  Google Cloud
   account, do not use it for this lab (avoids incurring charges).

4. Click through the subsequent pages:

   - Accept the terms and conditions.

   - Do not add recovery options  or two-factor authentication (because this is
     a temporary account).

   - Do not sign up for free trials.

After a few moments, the Cloud Console opens in this tab.

** Create an Actions project
Regardless of the  Assistant application you're building, you  will always have
to create an Actions project so your app has an underlying organizational unit.

Open the Actions  on [[http://console.actions.google.com/][Google Developer Console]]  in a new tab. Sign  in with your
Qwiklabs credentials  if prompted.  You should  be looking  at a  clean Actions
console that resembles the following:

Click *New  Project* and  agree to  Actions on Google's  terms of  service when
prompted by clicking *Agree and continue*.

Click  into the  =Project Name=  field and  select your  Qwiklabs Google  Cloud
project ID from the dropdown. Then click *Import project*:

Soon  after you  will  be presented  with  a welcome  page  that resembles  the
following:

Now  click  *Actions  Console*  in  the  top  left  corner  to  return  to  the
homepage. Then click on the project you just created (title has your Project ID
as the name.)

** Build An Action
An /action/  is an interaction you  build for the Google  Assistant.  An action
supports a  specific /intent/ (a goal  or task that users  want to accomplish),
which is  carried out by a  corresponding /fulfillment/ (logic that  handles an
intent and carries out the corresponding  Action.) You will now build an Action
that supports silly name generation.

Click on your project name. Then from the center menu click:
: Build your Action > Add Action(s) > Get Started

Then select
: Custom Intent > BUILD:

This will take you to the  Dialogflow console. Select your Qwiklabs account and
click *Allow* when Dialogflow prompts you  for permission to access your Google
Account.

When you land  on the Dialogflow account  settings page, check the  box next to
*Yes, I have read and accept the agreement* and click *Accept*.

If  you are  brought to  the following  Dialogflow agent  creation page,  click
*CREATE*:

If you are brought to this page instead:

Close  the Dialogflow  agent  creation  tab. You  will  return  to the  Actions
Console.

Click
: Get Started > Custom Intent > BUILD.

Select your Qwiklabs account and click  *Allow* when Dialogflow prompts you for
permission to access your Google Account.

Now click CREATE:

An /agent/ is an organizational unit that collects information needed to complete
a user's request, which it then forwards to a service that provides fulfillment
logic.

You will  now build  the basic  framework for fulfillment  logic. This  will be
handled (later) by a Cloud Function, which will return a response with a user's
silly name.

*** Test Completed Task
Click *Check my progress* to verify your performed task.

Click *Fulfillment* from  the left-hand menu. Move the slider  for *Webhook* to
the right, setting it to *Enabled*.

Now enter the  temporary URL https://google.com in for the  URL field. You will
update this URL  when you build your Cloud Function.  Your page should resemble
the following:

Scroll down and click Save in the  bottom right corner. Then click Intents from
the left hand menu and select Default Welcome Intent:

You will  now build the main  entry point into your  application by configuring
the default welcome intent.

** Configure the default welcome intent

** Design the conversation

** Configure a custom intent

** Initialize and configure a Cloud Function

** Configure the webhook

** Test your Assistant application with the Actions simulator

** Optional: test your application on a Google Home device

** Congratulations!

* Cloud Source Repositories---Qwik Start
:PROPERTIES:
:CUSTOM_ID: GSP121
:END:
- https://www.cloudskillsboost.google/focuses/1002?parent=catalog

** Overview
[[https://cloud.google.com/source-repositories/][Google  Cloud  Source Repositories]]  provides  Git  version control  to  support
collaborative development of any application or  service. 

In this  lab, you  will
- create a  local Git repository  that contains  a sample file,
- add a Google Source Repository as  a remote, and
- push the contents of the local  repository.
- You will use the source browser  included in Source Repositories to view your
  repository files from within the Cloud Console.

** Setup and Requirements

** Create a new repository

** Clone the new repository into your Cloud Shell session

** Push to the Cloud Source Repository

** Browse files in the Google Cloud Source repository

** View a file in the Google Cloud repository

** Test your Understanding

** Congratulations!

* Build a Serverless App with Cloud Run that Creates PDF Files
:PROPERTIES:
:CUSTOM_ID: GSP644
:END:
- https://www.cloudskillsboost.google/focuses/8390?parent=catalog

** Overview

** Setup and Requirements

** Understanding the task

** Enable the Cloud Run API

** Deploy a simple Cloud Run service

** Trigger your Cloud Run service when a new file is uploaded

** See if the Cloud Run service is triggered when files are uploaded to Cloud Storage

** Docker containers

** Testing the pdf-conversion service

** Congratulations!

* Firebase Web
:PROPERTIES:
:CUSTOM_ID: GSP065
:END:
- https://www.cloudskillsboost.google/focuses/660?parent=catalog

** Overview

** Setup and requirements

** Get the sample code

** View the starter application

** Set up your Firebase project

** Enable Firebase on your project

** Add a Firebase web app

** Install the Firebase command line interface

** Deploy and run the starter app

** Import and Configure Firebase

** Set up user sign in

** Write messages to Cloud Firestore

** Read messages

** Send Images

** Show Notifications

** Cloud Firestore security rules (optional)

** Cloud Storage security rules (optional)

** Deploy your app using Firebase Hosting

** Congratulations!

* About Hugo Static Site Generator
The world’s fastest framework for building  websites.  Hugo is not your average
static site generator.

- [[https://gohugo.io][Hugo Home]]
- [[https://github.com/gohugoio/hugo][Hugo on GitHub]]

Hugo is a fast and modern static  site generator written in Go, and designed to
make website creation fun again.

Hugo is  one of the most  popular open-source static site  generators. With its
amazing speed and flexibility, Hugo makes building websites fun again.

Improved performance, security  and ease of use  are just a few  of the reasons
static site generators are so appealing.

Hugo v0.15 and later are released under the Apache 2.0 license.

** Features

- Speed

  Hugo is  the fastest tool of  its kind. At <1  ms per page, the  average site
  builds in less than a second.

- Shortcodes

  Hugo's shortcodes are Markdown's hidden superpower.

  We love  the beautiful simplicity of  markdown’s syntax, but there  are times
  when we  want more  flexibility. Hugo  shortcodes allow  for both  beauty and
  flexibility.

- Multilingual and i18n

  Polyglot baked in.

  Hugo  provides full  i18n  support  for multi-language  sites  with the  same
  straightforward  development experience  Hugo users  love in  single-language
  sites.

- Robust Content Management

  Flexibility rules. Hugo is a content strategist's dream.

  Hugo supports unlimited content  types, taxonomies, menus, dynamic API-driven
  content, and more, all without plugins.

- Built-in Templates

  Hugo has common patterns to get your work done quickly. 300+ Themes

  Hugo ships  with pre-made templates  to make  quick work of  SEO, commenting,
  analytics and other functions. One line of code, and you're done.

  Hugo provides a  robust theming system that is easy  to implement but capable
  of producing even the most complicated websites.

- Custom Outputs

  HTML not enough?

  Hugo allows you to output your content in multiple formats, including JSON or
  AMP, and makes it easy to create your own.

- Capable Templating

  Hugo's Go-based templating  provides just the right amount of  logic to build
  anything from the simple to complex.

** Runtime Security
Hugo  produces  static output,  so  once  built,  the  runtime is  the  browser
(assuming the output is HTML) and any server (API) that you integrate with.

But  when  developing  and  building  your site,  the  runtime  is  the  ~hugo~
executable. Securing a runtime can be a real challenge.

#+texinfo:@heading Hugo’s main approach is that of sandboxing

- Hugo has  a virtual file  system and only  the main project  (not third-party
  components)  is allowed  to mount  directories or  files outside  the project
  root.

- Only the main project can walk symbolic links.

- User-defined components have only read-access to the filesystem.

- We shell  out to some external  binaries to support Asciidoctor  and similar,
  but those binaries  and their flags are predefined. General  functions to run
  arbitrary  external OS  commands  have been  discussed,  but not  implemented
  because of security concerns.

#+texinfo:@heading Dependency Security 

Hugo builds as a static binary using  Go Modules to manage its dependencies. Go
Modules have several safeguards, one of them being the ~go.sum~ file. This is a
database of the  expected cryptographic checksums of all  of your dependencies,
including any transitive.

Hugo Modules is  built on top of  Go Modules functionality, and  a Hugo project
using Hugo Modules will have a ~go.sum~ file. We recommend that you commit this
file to your  version control system.  The  Hugo build will fail if  there is a
checksum mismatch, which would be an indication of dependency tampering.

** Install Hugo
Install Hugo  on macOS, Windows,  Linux, OpenBSD,  FreeBSD, and on  any machine
where the Go compiler tool chain can run.

Hugo is written  in Go with support for multiple  platforms. The latest release
can be found at [[https://github.com/gohugoio/hugo/releases][Hugo Releases]].

Hugo currently provides pre-built binaries for the following:

- macOS (Darwin) for x64, i386, and ARM architectures
- Windows
- Linux
- OpenBSD
- FreeBSD

Hugo may also be compiled from source  wherever the Go toolchain can run; e.g.,
on other operating systems such as DragonFly BSD, OpenBSD, Plan 9, Solaris, and
others.

*** Quik Install

**** Binary
Download the  appropriate version  for your platform  from Hugo  Releases. Once
downloaded, the binary can  be run from anywhere. You don’t  need to install it
into a  global location.  This works  well for shared  hosts and  other systems
where you don’t have a privileged account.

Ideally,   you  should   install  it   somewhere  in   your  =PATH=   for  easy
use. ~/usr/local/bin~ is the most probable location.

**** Docker
We currently do not offer official Hugo  images for Docker, but we do recommend
these up to date distributions: https://hub.docker.com/r/klakegg/hugo/

**** Homebrew
If you are on macOS and using Homebrew, you can install Hugo with the following
one-liner:

: brew install hugo

**** MacPorts
If you are on macOS and using MacPorts, you can install Hugo with the following
one-liner:

: port install hugo

**** Homebrew Linux
If you  are using Homebrew  on Linux, you can  install Hugo with  the following
one-liner:

: brew install hugo

*** Source

**** Prerequisite Tools
- Git
- Go (>= v1.11)

**** Fetch from GitHub
Since  Hugo 0.48,  Hugo uses  the  Go Modules  support  built into  Go 1.11  to
build. The easiest way  to get started is to clone Hugo  in a directory outside
of the =GOPATH=, as in the following example:

#+caption: Install Hugo from Source on GitHub
#+name:from-gh.sh
#+begin_src sh
  mkdir $HOME/src
  cd $HOME/src
  git clone https://github.com/gohugoio/hugo.git
  cd hugo
  go install --tags extended
  # Remove --tags extended if you do not want/need Sass/SCSS support.
#+end_src

** Basic Usage
The following is a  description of the most common commands  you will use while
developing  your   Hugo  project.
- See the [[https://gohugo.io/commands/][Command Line Reference]] for a comprehensive view of Hugo’s CLI.

*** Make Sure Hugo Is Installed
Once you  have installed Hugo, make  sure it is  in your =PATH=.  You  can test
that Hugo has been installed correctly via the help command:

: hugo help

You should see some help output.

*** Hugo Command
The most  common usage is  probably to run  ~hugo~ with your  current directory
being the input directory.

This generates your website to the ~public/~ directory by default, although you
can customize the  output directory in your site configuration  by changing the
~publishDir~ field.

The command  ~hugo~ renders  your site into  ~public/~ dir and  is ready  to be
deployed to your web server:

#+begin_example
hugo
0 draft content
0 future content
99 pages created
0 paginator pages created
16 tags created
0 groups created
in 90 ms
#+end_example

*** Draft-Future-and Expired Content
Hugo allows  you to set =draft=,  =publishdate=, and even =expirydate=  in your
content’s front matter.  By default, Hugo will not publish:

1. Content with a future =publishdate= value
2. Content with =draft: true= status
3. Content with a past =expirydate= value

All  three  of these  can  be  overridden  during  both local  development  and
deployment  by  adding  the  following  flags  to  ~hugo~  and  ~hugo  server~,
respectively, or by  changing the boolean values assigned to  the fields of the
same name (without =--=) in your configuration:

1. =--buildFuture=
2. =--buildDrafts=
3. =--buildExpired=

*** Live Reload
Hugo comes  with *LiveReload* built  in.  There  are no additional  packages to
install.  A common way to use Hugo while  developing a site is to have Hugo run
a server with the ~hugo server~ command and watch for changes:

#+begin_example
hugo server
0 draft content
0 future content
99 pages created
0 paginator pages created
16 tags created
0 groups created
in 120 ms
Watching for changes in /Users/yourname/sites/yourhugosite/{data,content,layouts,static}
Serving pages from /Users/yourname/sites/yourhugosite/public
Web Server is available at http://localhost:1313/
Press Ctrl+C to stop
#+end_example

This will run a fully functioning web server while simultaneously watching your
file system for additions, deletions, or  changes within the following areas of
your project organization:

- =/static/*=
- =/content/*=
- =/data/*=
- =/i18n/*=
- =/layouts/*=
- =/themes/<CURRENT-THEME>/*=
- =config=

Whenever  you make  changes,  Hugo  will simultaneously  rebuild  the site  and
continue to serve  content. As soon as the build  is finished, LiveReload tells
the browser to silently reload the page.

Most Hugo builds are so fast that  you may not notice the change unless looking
directly at the site in your browser.  This means that keeping the site open on
a second monitor  (or another half of  your current monitor) allows  you to see
the most up-to-date version of your website without the need to leave your text
editor.

#+begin_cartouche
Hugo  injects  the LiveReload  <script>  before  the  closing </body>  in  your
templates and will therefore not work if this tag is not present..
#+end_cartouche

**** Redirect automatically to the page you just saved
When you are working with more than one  document and want to see the markup as
real-time as possible it’s not ideal  to keep jumping between them. Fortunately
Hugo  has an  easy,  embedded and  simple  solution for  this.

- It’s the  flag ~--navigateToChanged~

**** Disable LiveReload
LiveReload works  by injecting  JavaScript into the  pages Hugo  generates. The
script creates  a connection from the  browser’s web socket client  to the Hugo
web socket server.

The following methods make it easy to disable LiveReload:

: hugo server --watch=false

Or…

: hugo server --disableLiveReload

The latter flag can be omitted by adding the following to the ~config~:

#+begin_src js
  {
     "disableLiveReload": true
  }
#+end_src

*** Deploy Your Website
After running ~hugo server~  for local web development, you need  to do a final
~hugo run~ without  the server part of  the command to rebuild  your site.  You
may then deploy your site by copying the ~public/~ directory to your production
web server.

Since Hugo generates  a static website, your site can  be hosted anywhere using
any  web server.   See  Hosting  and Deployment  for  methods  for hosting  and
automating deployments contributed by the Hugo community.

#+begin_cartouche
Running ~hugo~  does not  remove generated files  before building.   This means
that you should  delete your ~public/~ directory (or the  publish directory you
specified via flag or configuration file) before running the ~hugo~ command. If
you do  not remove  these files,  you run the  risk of  the wrong  files (e.g.,
drafts or future posts) being left in the generated site.
#+end_cartouche

* About Google Cloud
- [[https://cloud.google.com][Google Cloud]]
- [[https://cloud.google.com/gcp/getting-started][Quick Starts]]


- Create a Linux VM
- Store a File and Share It
- Deploy a Docker Container Image
- Train a TensorFlow Model
- Run Label Detection on an Image
- Deploy a Python Application on App Engine

#+texinfo: @heading Bookshelf App
#+texinfo: @subheading The Bookshelf app is a sample web app that shows how to use a variety of Google Cloud products, including:

- App Engine flexible environment
- Cloud Storage
- Cloud SQL
- Compute Engine
- Datastore


#+texinfo: @heading Developer and Management Tools
#+texinfo: @subheading Tools and libraries to enhance developer productivity on Google Cloud.

- Cloud SDK :: [[https://cloud.google.com/sdk][Cloud SDK Link]]

  Command-line interface for Google Cloud products and services.

- Cloud Shell :: [[https://cloud.google.com/shell][Cloud Shell Link]]

  Manage  your infrastructure  and applications  from the  command-line in  any
  browser.

- Cloud Console :: [[https://cloud.google.com/cloud-console][Cloud Console Link]]

  Your integrated Google Cloud management console.

** About Google Cloud Source Repositories
*Cloud  Source Repositories*  are  private Git  repositories  hosted on  Google
Cloud. These  repositories let you  develop and deploy an  app or service  in a
space that provides collaboration and version control for your code.

- [[https://cloud.google.com/source-repositories][Google Cloud Source Repositories]]
- [[https://cloud.google.com/source-repositories/docs][Documentation]]
- [[https://console.cloud.google.com/freetrial/signup/tos?_ga=2.40204066.101294923.1636554500-1996190523.1636554500][Create an Account]]

A single place for your team to store, manage, and track code.

- Design, develop, and securely manage your code
- Collaborate easily on a fully featured, scalable, and private Git repository
- Extend your Git workflow by connecting to other Google Cloud tools

#+texinfo: @heading Benefits
- Unlimited private Git repositories ::

  Get free  unlimited private  repositories to organize  your code  however you
  wish. Mirror code from GitHub or  Bitbucket repositories to get powerful code
  search, code browsing, and diagnostics capabilities.

- Improve developer productivity ::

  Get fast feedback  on code changes with built-in  continuous integration. You
  can easily set up triggers to  automatically build and test using Cloud Build
  when you push changes to Cloud Source Repositories.

- Fast code search ::

  Use  powerful regexp  to  search  across multiple  directories.  You can  use
  regular expressions to refine your search or perform a single targeted search
  across projects, files, and code repositories.

#+texinfo: @heading Key Features
- Source Browser ::

  View repository files from within  the Cloud Source Repositories using Source
  Browser. Filter your view to focus on a specific branch, tag, or commit.

- Perform Git Operations ::

  Set up a repository  as a Git remote. Push, pull, clone  and log, and perform
  other Git operations required by your workflow.

- Automatic Syncing ::

  Connect  Cloud  Source Repositories  to  a  hosted  repository on  GitHub  or
  Bitbucket.  Automatically  sync changes  to  Cloud  Source Repositories  when
  changes are pushed to GitHub or Bitbucket.

- Proven Reliability ::

  Manage your code  on systems distributed geographically  across multiple data
  centers and being run on Google’s infrastructure with high availability.

** About Google Cloud Build
Build, test, and deploy on our serverless CI/CD platform.

- [[https://cloud.google.com/build][Google Cloud Build]]
- [[https://cloud.google.com/build/docs][Documentation]]

Cloud Build  is a service  that executes your  builds on Google  Cloud Platform
infrastructure. Cloud  Build can import  source code from Cloud  Storage, Cloud
Source  Repositories,   GitHub,  or   Bitbucket,  execute   a  build   to  your
specifications,  and  produce  artifacts  such as  Docker  containers  or  Java
archives.

Cloud Build executes  your build as a  series of build steps,  where each build
step is  run in a Docker  container. A build step  can do anything that  can be
done from a  container irrespective of the environment. To  perform your tasks,
you can either use  the supported build steps provided by  Cloud Build or write
your own build steps.

- Build

- Deploy

- Automate

- Build software quickly across all  programming languages, including Java, Go,
  Node.js, and more

- Choose from 15 machine types and run hundreds of concurrent builds per pool

- Deploy across multiple  environments such as VMs,  serverless, Kubernetes, or
  Firebase

- Access  cloud-hosted,  fully  managed  CI/CD workflows  within  your  private
  network

- Keep your data at rest within a geographical region or specific location with
  data residency

#+texinfo: @heading Benefits
- Fully serverless platform ::

  Cloud  Build  scales  up  and  scales down  with  no  need  to  pre-provision
  servers. Pay only for what you use. With private pools, you get access to the
  same serverless benefits within your own private network

- Flexibility ::

  With  custom build  steps  and pre-created  extensions  to third-party  apps,
  enterprises can  easily tie  their legacy  or home-grown tools  as a  part of
  their build process.

- Security and compliance ::

  Scan  for  vulnerabilities  as  part   of  your  CI/CD.  Automatically  block
  deployment of  vulnerable images. Set up  a secure CI/CD perimeter  and block
  public IPs with built-in support for VPC peering and VPC-SC.

#+texinfo: @heading Key Features
- Extremely fast builds ::

  Access machines connected via Google’s global network to significantly reduce
  your build time. Run builds on high-CPU  VMs or cache source code, images, or
  other dependencies to further increase your build speed.

- Automate your deployments ::

  Create   pipelines   as   a   part   of  your   build   steps   to   automate
  deployments. Deploy using built-in  integrations to Google Kubernetes Engine,
  App Engine, Cloud Functions, and Firebase. Use Spinnaker with Cloud Build for
  creating and executing complex pipelines.

- Support for multicloud ::

  Deploy to multiple clouds as a part of your CI/CD pipeline. Cloud Build comes
  with   builder    images   which    have   languages   and    tools   already
  installed. Likewise,  containerized tasks of  Cloud Build are  fully portable
  across different clouds.

- Commit to deploy in minutes ::

  Going from PR to build, test, and deploy can’t be simpler. Set up triggers to
  automatically build,  test, or deploy  source code  when you push  changes to
  GitHub, Cloud Source Repositories, or a Bitbucket repository.

- Unparalleled privacy ::

  Run  builds on  infrastructure protected  by Google  Cloud security.  Trigger
  fully managed CI/CD workflows from private source code repositories hosted in
  private networks, including GitHub Enterprise.

* Build Tools
:PROPERTIES:
:appendix: t
:custom_id: build-tools
:END:
** Makefile					:dependencies:env_vars:perl:
:PROPERTIES:
:appendix: t
:dependency1: make
:dependency2.0: AWS User account at https://aws.amazon.com
:dependency2.1: AWS cli v2 in PATH https://docs.aws.amazon.com/cli/index.html > which aws
:dependency2.2: See how to install AWS CLI v2 at https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-mac.html
:dependency2.3: See how to update AWS CLI v2 at https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-mac.html#cliv2-mac-install-cmd-all-users
:dependency2.4: See how to configure AWS CLI v2 at https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html
:dependency2.5: AWS credentials: access key id and secret access key stored in ~/.aws/credentials
:dependency2.6: AWS configuration stored in ~/.aws/config
:dependency2.7: AWS S3 buckets set up for serving a static web page
:dependency3: GitHub Account with personal access token stored in GITHUB_TOKEN
:dependency4: texinfo @6.7 or greater installed
:dependency5: Emacs with Org-mode and Babel language 'shell' enabled: '(org-babel-load-languages '(... (shell . t)))'
:env_var1: SYNC_ORG_TEMPLATE: holds the full path to this Template.org file
:env_var2: GITHUB_TOKEN: holds the GitHub personal access token
:env_var3: EMACS: reference to the Emacs executable
:env_var4: EDITOR: reference to a working emacsclient server
:env_var5: COLORS
:END:

#+pindex:Makefile
#+name:Makefile
#+header: :tangle Makefile
#+begin_src makefile
  
  ###############################################################################
  ### USER-DEPENDENT VARIABLES
  ### USE ENVIRONMENT VARIABLES WHENEVER POSSIBLE
  
  # NOTE: All environment variables need to be exported PRIOR to starting the
  # Emacs server as EDITOR in your shell startup files; otherwise, they will not
  # be available to Emacs.
  # When I moved from using Bash to Zsh, I inadvertently changed the order of
  # import, and started the Emacs server before importing, and caused a horrible
  # bug which caused the program to work on one computer but fail on another.
  
  # The absolute path to this Template file
  TEMPLATE := $(SYNC_ORG_TEMPLATE)
  
  # Use emacsclient as $EDITOR; make sure it is set in a shell startup file and
  # the server has been started.
  EMACS		:= $(EMACS)
  EDITOR	:= $(EDITOR)
  
  # User’s personal GitHub token for authentication to GitHub
  # DO NOT HARD-CODE THIS VALUE
  GITHUB_TOKEN := $(GITHUB_TOKEN)
  
  # The AWS Command Line Interface (AWS CLI) is an open source tool
  # that enables you to interact with AWS services using commands in
  # your command-line shell.  It must be present on your system.  Run the 'make'
  # command 'install-aws-cli' to install it if you do not have it.  Be sure to
  # run 'aws configure' after installing it.  This will place your AWS
  # credentials into ~/.aws/credentials.
  AWS := aws
  S3  := $(AWS) s3
  CFD := $(AWS) cloudfront
  
  ### END OF USER-DEPENDENT VARIABLES
  ###############################################################################
  ### MAKE-GENERATED VARIABLES
  
  ### TOOLS & RESOURCES
  # resources is a directory holding static resources for the project;
  # resources is created as a subdirectory of every new project.
  # resource/tools is a directory holding tangled scripts, such as cmprpl
  # resources/images is a directory holding jpg and png image files
  RESOURCES	:= resources
  TOOLS		:= $(RESOURCES)/tools
  IMAGES	:= $(RESOURCES)/images
  CMPRPL	:= $(TOOLS)/cmprpl
  
  ### PROJ AND ORG
  # ORG is the name of this Org file with extension .org
  # PROJ is the project name---the Org file name without extension.
  
  ### NOTE: there can be only one Org file in the project directory;
  # so far this has not been a problem, but it might be.
  
  PWD  := $(shell pwd)
  ORG  := $(shell ls *.org)
  PROJ := $(basename $(ORG))
  
  ### NOTE: S is needed only for the Template file because of the way it is nested
  # one level deep in the Templates GitHub repo, which uses the plural form
  # of Templates, whereas this file uses the singular form, Template.  So when
  # the homepage link is updated, the curl command must be told to use the plural
  # form.	 This is obviously a hack only for my own use and can be removed once
  # I clean up this anomaly.
  
  ifeq ($(PROJ),$(basename $(notdir $(TEMPLATE))))
  S := s
  endif
  
  # The AWS S3 bucket to use to store the html source file; it is found at the
  # key #+bucket towards the beginning of the file and should include the appropriate
  # suffix (.com, .net, .org, etc)
  BUCKET       := $(shell $(EDITOR) --eval \
		 '(with-current-buffer (find-file-noselect "$(ORG)") \
		    (save-excursion \
		      (goto-char (point-min)) \
		      (re-search-forward "^\#[+]bucket:\\(.*\\)$$" nil t) \
		      (match-string-no-properties 1)))')
  S3_BUCKET    := s3://$(BUCKET)
  
  # Buckets set up to serve static web sites from S3 can use either http
  # or https protocols; some  http protocols will automatically redirect
  # to https;  however, some only use  http. I would like  to accomodate
  # both, and  so this code  finds the url's  that are in  my Cloudfront
  # account, which presumably will serve https.  If the url is not here,
  # then this must be set up to serve http instead.
  HTTP_S := $(shell $(CFD) list-distributions | perl -MJSON::PP -e \
	  '$$/=""; \
	   my @urls = (); \
	   my $$json=JSON::PP->new->decode(<STDIN>); \
	   for my $$item ( @{$$json->{"DistributionList"}{"Items"}} ) { \
		  push @urls, @{$$item->{"Aliases"}{"Items"}}; \
	   } \
	  my $$found = grep { /'$(BUCKET)'/ } @urls; \
	  print "http", ($$found ? "s" : "");')
  
  HTTPS_BUCKET := https://$(BUCKET)
  
  ### DIR, SRC
  # DIR is the .info name found at '#+texinfo_filename:<DIR>.info' (at
  # the bottom of this file in the export configuration settings)
  # without its extension, used as the INFO filename and the name of the
  # HTML export directory; this code uses the lowercased PROJ name if
  # there is no '#+texinfo_filename'.
  # SRC is HTML directory based upon the DIR name
  
  #DIR := $(shell $(EDITOR) --eval \
  #	'(with-current-buffer (find-file-noselect "$(ORG)") \
  #		(save-excursion \
  #		(goto-char (point-min)) \
  #		(re-search-forward "^\#[+]\\(?:texinfo_filename\\|TEXINFO_FILENAME\\):\\(.*\\).info$$" nil t) \
  #		(match-string-no-properties 1)))')
  
  DIR := $(shell sed -E -n "/^\#\+texinfo_filename/s/^.*:(.*)\.info$$/\1/p" $(ORG))
  ifeq ($(DIR),$(EMPTY))
	  DIR := $(shell echo $(PROJ) | tr "[:upper:]" "[:lower:]")
  endif
  
  SRC := $(DIR)/
  
  ### VERS: v1.2.34/
  # VERS is the version number of this Org document.
  # When sync is run after the version number has been updated, then VERS
  # picks up the newly-changed value.  VERS used to be staticly imbedded
  # when the Makefile was tangled, but it needs to be dynamic for
  # development.
  
  # QUERY: should this number be formatted like this, or should it be just the numbers?
  # The reason it includes them is the S3PROJ obtains the name from the S3 bucket, and
  # it includes them.  But it only includes them because I have made it so.  Not a good
  # reason just by itself.  The ending slash is not actually a part of the version, but
  # comes from the way the 'aws2 ls' command returns its values.	So VERS should probably
  # not include the trailing slash, although it doesn’t hurt anything.
  
  VERS := v$(shell $(EDITOR) --eval \
	  '(with-current-buffer (find-file-noselect "$(ORG)") \
		  (save-excursion \
		    (goto-char (point-min)) \
		    (re-search-forward "^\#[+]\\(?:macro\\|MACRO\\):version Version \\(\\(?:[[:digit:]]+[.]?\\)\\{3\\}\\)") \
		    (match-string-no-properties 1)))')/
  
  ### AWS
  # PROJ_LIST contains the list of projects currently uploaded to
  # the S3 bucket; each item contains the name of the project and its
  # current version.
  
  # Created function using elisp instead of the shell.
  # This variable contains an elisp list of strings of the form '("proj1-v1.2.3/" "proj2-v4.5.6/" ...)'
  # However, when it prints to the shell, the quotes are lost.
  # Need to make sure elisp's variable 'exec-path contains the proper $PATH instead of adding to 'exec-path.
  
  PROJ_LIST := $(shell $(EDITOR) --eval \
	  "(progn \
		  (require (quote seq)) (add-to-list (quote exec-path) (quote \"/usr/local/bin\")) \
		  (seq-map (lambda (s) (replace-regexp-in-string \"^\s+PRE \" \"\" s)) \
			  (seq-filter (lambda (s) (string-match-p (regexp-quote \" PRE \") s)) \
			  (process-lines \"$(AWS)\" \"s3\" \"ls\" \"$(S3_BUCKET)\"))))")
  
  ### S3PROJ
  # The name of the current project as obtained from S3: 'proj-v1.2.34/'
  # If there is no current project in the S3 bucket, then assign a value equal to
  # the Org project and version instead.  It is set to the project if found, and
  # NO if not found, then updated in the ifeq block below.
  S3PROJ := $(shell $(EDITOR) --eval \
		  '(let ((proj (seq-find (lambda (s) (string-match-p "$(DIR)" s)) (quote $(PROJ_LIST))))) \
		     (or proj (quote NO)))')
  
  ### PROJINS3
  # is used by make sync; this allows the index.html file to be generated the first
  # time the project is synced.  It is set to NO if this project is not currently in an
  # S3 bucket, and it is set to YES if it is.
  PROJINS3 :=
  
  ### S3VERS
  # The version of this project currently installed in the S3 bucket: 'v1.2.34/'
  # If there is no current version in the S3 bucket, then assign the version from
  # this Org file instead.
  S3VERS   :=
  
  # Update S3PROJ, S3VERS, and PROJINS3
  ifeq ($(S3PROJ), NO)
	  S3PROJ := $(DIR)-$(VERS)
	  S3VERS := $(VERS)
	  PROJINS3 := NO
  else
	  S3VERS := $(subst $(DIR)-,,$(S3PROJ))
	  PROJINS3 := YES
  endif
  
  ### GITHUB
  # USER is the current user's GitHub login name.
  
  # The user name used to be statically embedded into the Makefile
  # during tangle, but in an effort to make the Makefile dynamically
  # indepedent, dynamic code has replaced the static code.  The code
  # that placed the static name in the Makefile was a 'node' script that
  # ran in a separate Org process during tangle.	An unfortunate fact of
  # 'make' is that 'make' strips the quote marks from the string
  # obtained from the 'curl' command when the 'make shell' command
  # returns the string.	 This makes the string malformed JSON and
  # unparsable by most JSON parsers, including 'node’.	However,
  # 'perl'’s core module JSON::PP (but not JSON::XS) has facilities to
  # parse very malformed JSON strings.	Therefore, this dynamic code
  # uses 'perl' and the core module JSON::PP to parse the 'curl' string
  # into a 'perl' JSON object which can return the login name.	This
  # code should work with any version of 'perl' without having to
  # install any modules.
  
  USER	:= $(shell \
	    curl -sH "Authorization: token $(GITHUB_TOKEN)" https://api.github.com/user \
	    | \
	    perl -MJSON::PP -e \
		'$$/ = ""; \
		 my $$json = JSON::PP->new->loose->allow_barekey->decode(<STDIN>); \
		 print $$json->{login};' \
	    )
  SAVE		:= resources
  
  ### TEXINFO
  TEXI		:= $(PROJ).texi
  INFO		:= $(DIR).info
  INFOTN		:= $(shell $(EDITOR) --eval "(file-truename \"$(INFO)\")")
  PDF		:= $(PROJ).pdf
  INDEX		:= index.html
  HTML		:= $(DIR)/$(INDEX)
  DIR_OLD		:= $(DIR)-old
  
  ### AWS S3
  DST_OLD		:= $(S3_BUCKET)/$(S3PROJ)
  DST_NEW		:= $(S3_BUCKET)/$(DIR)-$(VERS)
  EXCL_INCL		:= --exclude "*" --include "*.html"
  INCL_IMAGES	:= --exclude "*" --include "*.jpg" --include "*.png" --include "*.pdf"
  GRANTS		:= --grants read=uri=http://acs.amazonaws.com/groups/global/AllUsers
  S3SYNC		:= $(S3) sync --delete $(EXCL_INCL) $(SRC) $(DST_OLD) $(GRANTS)
  S3MOVE		:= $(S3) mv --recursive $(DST_OLD) $(DST_NEW) $(GRANTS)
  S3COPY		:= $(S3) cp $(INDEX) $(S3_BUCKET) $(GRANTS)
  S3REMOVE		:= $(S3) rm $(S3_BUCKET)/$(S3PROJ) --recursive
  S3IMAGESYNC	:= $(S3) sync $(INCL_IMAGES) $(IMAGES) $(S3_BUCKET)/$(IMAGES) $(GRANTS)
  
  ###############################################################################
  
  default: check texi info html pdf
  
  PHONY: default all check values boot \
	    texi info html pdf \
	    open-org open-texi open-html open-pdf \
	    clean dist-clean wiped-clean \
	    help sync update delete-proj \
	    install-aws-cli \
	    index-html upload-index-html
  
  values: check
	    @printf "$${BLUE}Values...$${CLEAR}\n"
	    @echo TEMPLATE:	$(TEMPLATE)
	    @echo EDITOR:	$(EDITOR)
	    @echo USER:		$(USER)
	    @echo PWD:		$(PWD)
	    @echo ORG:		$(ORG)
	    @echo TEXI:		$(TEXI)
	    @echo INFO:		$(INFO)
	    @ECHO INFOTN:	$(INFOTN)
	    @echo BUCKET:	$(BUCKET)
	    @echo PROJ:		$(PROJ) $S
	    @echo S3_BUCKET:	$(S3_BUCKET)
	    @echo HTTP_S:	$(HTTP_S)
	    @echo HTTPS_BUCKET:	$(HTTPS_BUCKET)
	    @echo VERS:		$(VERS)
	    @echo S3PROJ:	$(S3PROJ)
	    @echo S3VERS:	$(S3VERS)
	    @echo DIR:		$(DIR)
	    @echo DIR_OLD:	$(DIR_OLD)
	    @echo SRC:		$(SRC)
	    @echo DST_OLD:	$(DST_OLD)
	    @echo DST_NEW:	$(DST_NEW)
	    @echo PROJ_LIST:	"$(PROJ_LIST)"
	    @echo PROJINS3:	$(PROJINS3)
  
  check:
	    @printf "$${BLUE}Checking dependencies...$${CLEAR}\n"
  
	    @[[ -z $(BUCKET) ]] && \
	       { printf "$${RED}$(BUCKET) $${CYAN}must be set.$${CLEAR}\n"; exit 1; } || \
	       printf "$${CYAN}BUCKET: $${GREEN}$(BUCKET)$${CLEAR}\n";
  
	    @[[ -z $${GITHUB_TOKEN} ]] && \
	       { printf "$${RED}GITHUB_TOKEN $${CYAN}must be set.$${CLEAR}\n"; exit 1; } || \
	       printf "$${CYAN}GITHUB_TOKEN: $${GREEN}SET$${CLEAR}\n";
  
	    @[[ (-d ~/.aws) && (-f ~/.aws/credentials) && (-f ~/.aws/config) ]] && \
	       printf "$${CYAN}AWS credentials and config: $${GREEN}SET$${CLEAR}\n" || \
	       { printf "$${RED}~/.aws 'credentials' and 'config' must be set.$${CLEAR}\n"; exit 1; }
  
	    @[[ "$(shell $(EDITOR) --eval '(member (quote texinfo) org-export-backends)')" = "(texinfo)" ]] && \
		  printf "$${CYAN}Texinfo backend: $${GREEN}INSTALLED.$${CLEAR}\n" || \
		  { printf "$${YELLOW}Texinfo backend:$${CLEAR} $${RED}NOT INSTALLED; it must be installed.$${CLEAR}\n"; exit 1; }
  
	    @[[ $(shell $(EDITOR) --eval '(symbol-value org-confirm-babel-evaluate)') == "t" ]] && \
		  { printf "$${YELLOW}org-confirm-babel-evaluate:$${CLEAR} $${RED}T; set to NIL.$${CLEAR}\n"; exit 1; } || \
		  printf "$${CYAN}org-confirm-babel-evaluate: $${GREEN}OFF.$${CLEAR}\n\n"
  
  open-org: $(ORG)
	    @$(EDITOR) -n $(ORG)
  $(ORG):
	    @echo 'THERE IS NO $(ORG) FILE!!!'
	    exit 1
  
  texi: $(TEXI)
  $(TEXI): $(ORG)
	   @echo Making TEXI...
	   @$(EDITOR) -u --eval \
		  "(with-current-buffer (find-file-noselect \"$(ORG)\" t) \
			  (save-excursion \
			  (org-texinfo-export-to-texinfo)))"
	   @echo Done making TEXI.
  open-texi: texi
	   @$(EDITOR) -n $(TEXI)
  
  info: $(INFO)
  $(INFO): $(TEXI)
	   @echo Making INFO...
	   @makeinfo -o $(INFO) $(TEXI)
	   @$(EDITOR) -u -eval \
		  "(when (get-buffer \"$(INFO)\") \
			  (with-current-buffer (get-buffer \"$(INFO)\") \
				  (revert-buffer t t t)))"
	   @echo Done making INFO.
  
  open-info: info
	   @$(EDITOR) -u -eval \
		  "(if (get-buffer \"*info*\") \
			  (with-current-buffer (get-buffer \"*info*\") \
				(when (not (string= \"(symbol-value (quote Info-current-file))\" \"$(INFOTN)\")) \
					(info \"$(INFOTN)\")) \
				(revert-buffer t t t)) \
		      (info \"$(INFOTN)\"))"
  
  html: $(HTML)
  $(HTML): $(TEXI)
	   @echo Making HTML INFO..
	   @makeinfo --html -o $(DIR) $(TEXI)
	   @echo Done making HTML.
	   $(CMPRPL) $(DIR) $(DIR_OLD)
  open-html: html
	   @open $(HTML)
  
  # If pdftexi2dvi produces an error, it may still produce a viable PDF;
  # therefore, use --tidy.  If it produces an error, try to link the PDF;
  # if it does not produce an error, the PDF will be added to the top dir
  # and there will be no attempt to link.
  pdf:	$(PDF)
  $(PDF): $(TEXI)
	  @echo Making PDF INFO...
	  @-pdftexi2dvi --quiet --build=tidy $(TEXI) || ln -s $(PROJ).t2d/pdf/build/$(PDF) $(PDF)
	  @echo Done making PDF.
  open-pdf:pdf
	   @open $(PDF)
  
  tangle: $(ORG)
	      @$(EDITOR) -u --eval "(org-babel-tangle)"
	      @echo Done tangling
  
  sync:   $(HTML)
	  @echo Syncing version $(VERS) onto $(S3VERS)...
	  $(S3SYNC)
	  $(S3IMAGESYNC)
	  @echo Done syncing.
	  [[ $(VERS) != $(S3VERS) ]] && { echo Moving...; $(S3MOVE); echo Done moving.;  make homepage; } || :
	  [[ $(PROJINS3) = "NO" ]] && make homepage || :
  
  # This is a target-specific variable for updating the “description”
  # key on the GitHub repo page with the current version number.  It
  # first makes a curl call to the GitHub project repo, finds the
  # “description” line, pulls out the description only (leaving the old
  # version) and then prints the value with the current version number.
  # This value is used by the “homepage:” target in the PATCH call.
  # This method is arguably harder to code but faster to run than using
  # Perl with the JSON::PP module.
  
  homepage: description = $(shell \
	  curl -s \
		  -H "Authorization: token $(GITHUB_TOKEN)" \
		  https://api.github.com/repos/$(USER)/$(PROJ)$S | \
		  (perl -ne 'if (/^\s*\"description\":\s*\"(.*): v(?:(?:[[:digit:]]+[.]?){3})/) {print $$1}'))
  
  ### NOTE the use of the S variable at the end of PROJ; this is to handle
  # the singular case of the GitHub repo using the plural form, Templates
  # whereas the the Template.org file uses the singular form.
  homepage: $(ORG) upload-index-html
	    @echo Updating homepage...
	    @echo DESCRIPTION: $(description)
	    @echo VERS: $(VERS)
	    @curl -i \
		  -H "Authorization: token $(GITHUB_TOKEN)" \
		  -H "Content-Type: application/json" \
		  -X PATCH \
		  -d "{\"homepage\":\"$(HTTPS_BUCKET)/$(DIR)-$(VERS)\",\
		       \"description\":\"$(description): $(VERS)\"}" \
		  https://api.github.com/repos/$(USER)/$(PROJ)$S
	    @echo Done updating homepage.
  
  delete-proj:
	  @echo Deleting project $(PROJ)...
	  @curl -i \
		  -H "Authorization: token $(GITHUB_TOKEN)" \
		  -H "Accept: application/vnd.github.v3+json" \
		  -X DELETE \
		  https://api.github.com/repos/$(USER)/$(PROJ)$S
	  @$(S3REMOVE)
	  @make dist-clean
	  @make upload-index-html
	  @$(EDITOR) -u --eval "(kill-buffer \"$(ORG)\")"
	  @rm -rf "../$(PROJ)"
	  @echo Done deleting project.
  
  index-html: $(INDEX)
  $(INDEX): $(ORG)
	  @echo making index.html...
	  $(EDITOR) --eval \
	  "(with-current-buffer (find-file-noselect \"$(ORG)\") \
		  (save-excursion \
		    (org-link-search \"#project-index-title\") \
		    (org-export-to-file (quote html) \"index.html\" nil t)))"
	  @echo Done making index.html.
  
  upload-index-html: $(INDEX)
	   @echo Uploading index.html...
	   $(S3COPY)
	   @echo Done uploading index.html
  
  install-aws-cli:
	    curl "https://awscli.amazonaws.com/AWSCLIV2.pkg" -o "AWSCLIV2.pkg" && \
	    sudo installer -pkg AWSCLIV2.pkg -target / && \
	    which aws && aws --version
	    rm -rf AWSCLIV2.pkg
  
  clean:
	  @echo Cleaning...
	    -@rm *~ 2>/dev/null
	    -@for file in *.??*; \
	    do \
		    ext=$${file#$(PROJ).}; \
		    [[ ! $${ext} =~ org|texi|info|pdf|html ]] && rm -rv $${file}; \
	    done
  
  dist-clean: clean
	  @echo Dist Cleaning...
	    @${EDITOR} -u --eval \
	      "(kill-buffer \"$(ORG)\")"
	    -@rm -rf *.{texi*,info*,html*,pdf*} $(DIR) $(TOOLS)
	    -@for dir in *; \
		do \
		    [ -d $$dir -a $$dir != "$(DIR_OLD)" -a $$dir != $(SAVE) ] && \
		    rm -vr $$dir; \
		done
  
  wipe-clean: dist-clean
	  @echo Wipe Clean...
	    -@rm -rf Makefile Readme.md $(DIR_OLD)
	    @git checkout Makefile README.md
  
  git-ready: dist-clean
	    git checkout Makefile
	    git checkout README.md
	    git status
  
  help:
	    @echo '"make boot" tangles all of the files in Template'
	    @echo '"make default" makes the .texi file, the .info file, \
	    the html files, and the .pdf file.'
	    @echo
  
	    @echo '"make check" checks for prerequistes'
	    @echo '"make values" runs check and prints variable values'
	    @echo
  
	    @echo '"make texi" makes the .texi file'
	    @echo '"make info" makes the .info file'
	    @echo '"make html" makes the html distribution in a subdirectory'
	    @echo '"make pdf" makes the .pdf file'
	    @echo
  
	    @echo '"make open-org" opens the ORG program using emacsclient for editing'
	    @echo '"make open-texi" opens the .texi file using emacsclient for review'
	    @echo '"make open-html" opens the distribution index.html file \
	    in the default web browser'
	    @echo '"make open-pdf" opens the .pdf file'
	    @echo
  
	    @echo '"make sync" syncs the html files in the AWS S3 bucket BUCKET; \
	    you must have your AWS S3 bucket name in the env var AWS_S3_BUCKET; \
	    You must have your AWS credentials installed in ~/.aws/credentials'
	    @echo
  
	    @echo '"make install-aws-cli" installs the "aws cli v2" command-line tools'
	    @echo 'You also need to run "aws configure" and supply your Access Key and Secret Access Key'
	    @echo
  
	    @echo '"make clean" removes the .texi, .info, and backup files ("*~")'
	    @echo '"make dist-clean" cleans, removes the html distribution, \
	    and removes the build directory'
	    @echo '"make wipe-clean" wipes clean the directory, including old directories'
	    @echo
  
	    @echo '"make delete-proj" deletes the project from the file system, GitHub and AWS'
  
#+end_src

*** TODO Next
1. The CloudFront configuration needs to be updated recognize the new version
   directory that is created as part of the ~sync~ operation.

2. Update the GitHub HOME website link for each new sync operation.

3. Store on GitHub a version of each other format upon a sync operation (i.e.,
   the INFO and PDF versions)

** Compare Replace

#+begin_comment
The following source code tangles all files during an export operation. This is
to  make  sure  the  ~cmprpl~  source code  exists  in  the  ~resources/tools/~
directory before running  the Makefile target =html=. It also  makes sure there
is a Makefile on an initial export. The following code is not exported.
#+end_comment

#+name:tangle-org-file
#+header: :exports results :eval yes :results silent
#+begin_src emacs-lisp
(org-babel-tangle-file (buffer-file-name))
#+end_src

The  AWS ~sync~  command  relies  upon time  stamps  to  determine whether  two
programs are identical or not, as  well as content.  If two otherwise identical
files have  different time stamps,  ~sync~ will  assume they are  different and
will  process the  newer.   However, the  ~texinfo~  ~makeinfo --html~  command
produces all  new files even  if some files  (or most files)  remain unchanged.
This  means that  all files  will be  uploaded to  the AWS  S3 bucket  on every
iteration, even though the majority of the files are actually unchanged.

The ~cmprpl~  source code attempts to  resolve the issue of  identical exported
code having different  time stamps, thus defeating the benefit  provided by the
~aws2 s3 sync~ command uploading only changed files.

This program makes sure that a generated HTML directory exists: =$DIR_NEW=.  If
it doesn’t, then it is in an improper state and the program stops with an error
message.

The  program then  checks  if  an old  directory  exists,  =$DIR_OLD=.  If  one
doesn’t,  then one  is  created by  copying the  current  new directory.   This
provides a baseline  for comparisons going forward.  The program  exits at that
point. It is very important that  the =$DIR_OLD= directory not be deleted going
forward.

Given  that =$DIR_OLD=  exists, the  program then  loops through  all files  in
=$DIR_NEW= and  compares them  to the  files in =$DIR_OLD=.   If the  files are
identical, the =$DIR_OLD= file replaces the =$DIR_NEW= file while retaining the
old time stamp (using the ~-p~ option of ~cp~. If a file is different, then the
=$DIR_NEW= file  replaces the =$DIR_OLD=  file, thus giving it  updated content
and  an updated  time stamp.   If the  file does  not exist  in the  =$DIR_OLD=
directory, then it is added.

The  program then  loops through  all of  the files  in the  old directory  and
deletes  any that  do not  exist in  the new  directory.  Now  both directories
should be in sync.

#+caption:Compare Replace program
#+name:cmprpl
#+header: :mkdirp t
#+header: :shebang "#!/usr/bin/env bash"
#+begin_src sh :tangle resources/tools/cmprpl
  [[ $# -eq 2 ]] || { echo "ERROR: Incorrect command line arguments"; exit 1; }
  DIR_NEW=$1
  DIR_OLD=$2

  [[ -d $DIR_NEW ]] || { echo "ERROR: $DIR_NEW does not exist"; exit 1; }
  [[ -d $DIR_OLD ]] || { echo "CREATING: $DIR_OLD does not exist"; cp -a $DIR_NEW $DIR_OLD; exit 0; }

  for newfile in $DIR_NEW/*
  do
      oldfile=$DIR_OLD/$(basename $newfile)
      if [[ -e $oldfile ]]
      then
	 if cmp -s $newfile $oldfile
	 then
	     printf "${GREEN}copying OLD to NEW${CLEAR}: "
	     cp -vp $oldfile $newfile
	 else
	     printf "${PURPLE}copying NEW to OLD${CLEAR}: "
	     cp -vp $newfile $oldfile
	 fi
      else
	  printf "${BLUE}creating NEW in OLD${CLEAR}: "
	  cp -vp $newfile $oldfile
      fi
  done

  for oldfile in $DIR_OLD/*
  do
      newfile=$DIR_NEW/$(basename $oldfile)
      if [[ ! -e $newfile ]]
      then
	  printf "${RED}removing OLD${CLEAR}: "
	  rm -v $oldfile
      fi
  done
#+end_src


** Update Utility Commands
*** Get Parsed Org Tree
This function looks for an Org file in the present working directory, and if it
finds one returns  a parsed tree using  ~org-element-parse-buffer~.  It returns
=nil= if there is no Org file or if the found file is not in ~org-mode~.

#+name:get-parsed-org-tree
#+header: :results silent
#+begin_src emacs-lisp
(defun get-parsed-org-tree (&optional org-dir)
  "This function takes an optional directory name, changes to
that directory if given, otherwise uses the pwd, and finds an Org
file and returns its parsed tree, or nil if none found."
  (when org-dir
      (cd (file-name-as-directory org-dir)))
  (let ((buf (car-safe (find-file-noselect "*.org" nil nil t))))
    (if buf
	(with-current-buffer buf (org-element-parse-buffer))
      nil)))
#+end_src

*** Check for CID
This code  checks whether an  Org file contains  a =custom_id= of  a particular
value.  It accepts  a ~cid-value~ and an optional directory.   If the directory
is not given, then it defaults to the current directory.  If throws an error if
the directory does not exist.  It returns =nil= if the given directory does not
contain an Org file.   It returns =t= if the Org file  contains a node property
of   =custom_id=  and   value  ~cid-value~,   or   =nil=  if   not.   It   uses
~get-parsed-org-tree~.

#+name:org-tree-cid-p
#+header: :results silent
#+begin_src emacs-lisp
(defun org-tree-cid-p (cid-value &optional org-dir)
  "Check whether an org file contains a custom_id of CID"
  (let ((tree (get-parsed-org-tree org-dir)))
    (car (org-element-map tree 'property-drawer
	   (lambda (pd) (org-element-map (org-element-contents pd) 'node-property
			  (lambda (np)
			    (and
			     (string= "custom_id" (org-element-property :key np))
			     (string= cid-value (org-element-property :value np))))))
	   nil t))))
#+end_src

#+name:run-org-tree-cid-p
#+header: :var cid="build-tools"
#+header: :var dir="/usr/local/dev/programming/MasteringEmacs"
#+header: :var gpot=get-parsed-org-tree()
#+header: :var otcp=org-tree-cid-p()
#+header: :results value
#+header: :eval never-export
#+begin_src emacs-lisp
(org-tree-cid-p cid dir)
#+end_src

#+call: run-org-tree-cid-p(dir="/usr/local/dev/programming/MasteringEmacs")

*** Keywords and Values
This function takes  an Org file name and optionally  a directory (otherwise it
uses the default  directory) and returns the  value of a keyword.   It does not
use a parse tree, but rather loops through the file line-by-line until it finds
the keyword and then returns its value.

#+name:get-keyword-value
#+begin_src emacs-lisp
  (defun get-keyword-value (keyword-to-get org-file-name &optional dir)
    "Returns the value of a keyword in an Org buffer identified by ORG-FILE-NAME.
  Uses the current directory unless an optional DIR is supplied.
  Returns NIL if none is found.  Rather than parsing the whole Org
  buffer into a tree, this function simply starts at the beginning
  of the file and loops line by line through the file, returning
  when the key has been found or it reaches the end of the file."
    (with-current-buffer
	(find-file-noselect
	 (concat
	  (if dir (file-name-as-directory dir) default-directory)
	  org-file-name))
      (save-excursion
	(goto-char (point-min))
	(let ((done nil)
	      (ans nil))
	  (while (not done)
	    (let* ((el (org-element-at-point))
		   (ty (org-element-type el))
		   (key (org-element-property :key el))
		   (val (org-element-property :value el)))
	      (when (and
		     (string-equal ty "keyword")
		     (string-equal key keyword-to-get))
		(setq ans val done t))
	      (forward-line)
	      (when (eobp)
		(setq done t))))
	  ans))))
#+end_src

#+name:get-title-for-org-buffer
#+begin_src emacs-lisp
(defun get-title-for-org-buffer (org-file-name &optional dir)
"A wrapper around `get-keyword-value' to find a TITLE in an Org buffer."
  (get-keyword-value "TITLE" org-file-name dir))
#+end_src

** Bucket Index HTML
The bucket should contain a master ~index.html~  file that links to each of the
individual project  ~index.html~ files.  The  master ~index.html~ file  will be
placed at the root of  the bucket, ~https://<bucket-name>.com/~, and the bucket
must be set up to serve this ~index.html~ when the user hits the root.

*** Get Bucket Name
 This  code searches  for  the keyword-value  pair =bucket:<BUCKET-NAME>=  that
 should be  located towards the  beginning of the  file, and returns  the value
 =BUCKET-NAME= or nil if not found.

#+name: get-bucket-name
#+header: :results value
#+begin_src emacs-lisp
   (save-excursion
     (goto-char (point-min))
     (re-search-forward "^#\\+bucket:\\s*?\\(.*\\)$" nil t)
     (match-string-no-properties 1))
#+end_src

For some reason, ~get-bucket-name~ does not  work when called from the headline
[[#project-index-links][=Links for  bucket=]] below  when creating  =index.html=, even  if it  returns as
~(prin1 ...)~ and is  set up to ~:return output~; the  call receives =nil=. The
following code from ~bucket-name~, however, works. I don't know why.

#+name: bucket-name
#+header: :results output
#+header: :var bucket-name=get-bucket-name()
#+begin_src emacs-lisp
(prin1 bucket-name)
#+end_src

*** Bucket HTTPS URL
This  code calls  ~get-bucket-name~ and  returns the  value returned  as a  URL
string or nil.

#+name: bucket-https-url
#+header: :results value
#+header: :var b=get-bucket-name()
#+begin_src emacs-lisp
(concat "https://" b)
#+end_src

*** S3 Bucket URL
This code calls ~get-bucket-name~ and returns the AWS S3 bucket url.

#+name: s3-bucket-url
#+header: :results value
#+header: :var b=get-bucket-name()
#+begin_src emacs-lisp
(concat "s3://" b)
#+end_src

*** Bucket Projects List
This code uses the ~s3-bucket-url~ result to obtain the list of projects in the
bucket.  It does  this by calling the  AWS S3 high-level command  ~ls~ and then
removing the  =PRE= string in  each result.  The result  that is returned  is a
single  string that  can be  separated into  individual links  by breaking  the
string on spaces.

#+name: bucket-projects-list
#+header: :results output
#+header: :var bucket=s3-bucket-url()
#+begin_src sh
/usr/local/bin/aws s3 ls ${bucket} | sed -ne 's/^.*PRE //p'
#+end_src

*** Bucket Project Links
This code  uses the result  from ~bucket-projects-list~ to create  an unordered
list of  links written to  bucket projects, written  in Org-mode syntax.  It is
executed by a =#+call:= in [[*Bucket Index][*Bucket  Index]] during an HTML export of that subtree
to a file called =index.html=.

#+name: bucket-project-links
#+header: :var b-url=bucket-https-url()
#+header: :var projects=bucket-projects-list()
#+header: :results output raw
#+begin_src emacs-lisp
(seq-do (lambda (u) (princ (format "- [[%s/%sindex.html][~%s~]]
" b-url u u))) (split-string projects))
#+end_src

*** Bucket Index
    :PROPERTIES:
    :custom_id: project-index-title
    :export_file_name: index.html
    :export_subtitle: {{{version}}} created {{{upload-date}}}
    :END:
#+html_doctype: html5
#+options: toc:nil html5-fancy:t

#+html: <hr>

**** Links for bucket call_bucket-name()
     :PROPERTIES:
     :unnumbered: t
     :custom_id: project-index-links
     :END:

#+call: bucket-project-links()
** Project Readme
This adds the README.md template to a project. It should be customized uniquely
for the project.

#+name:project-readme
#+header: :tangle README.md
#+begin_src markdown
# TITLE
## Subtitle
## Author
## Date
## Version
# ABSTRACT
This is the Org Template file.	It is the parent of all other Org Info blogs,
and provides the source code for processing them in various different ways.
# INTRODUCTION
# CHAPTER
## Section
### Subsection
#+end_src

** Boot Template
:PROPERTIES:
:dependency1: EMACS:=:/Applications/MacPorts/Emacs.app/Contents/MacOS/Emacs or similar
:dependency2: EDITOR:=:emacsclient
:dependency3: =SYNC_ORG_TEMPLATE= defined as $DEV/Templates/Org/Template.org
:END:
Although running the command ~org-babel-tangle~ (=C-c C-v t=) from within Emacs
will install  everything, it would  be nice to have  a simple Makefile  that is
downloaded with this  file that could be  invoked to do the  same thing without
starting Emacs and Org-mode and keying in the ~org-babel-tangle~ command.  This
little Makefile should be stored on  GitHub along with the ~Template.org~ file.
When  the source  is extracted  to a  directory, then  running this  Makefile's
default rule  as simply ~make~  will extract the ~preprocess.el~  script, which
updates  =DEV= and  then  extracts the  full Makefile.   Because  this file  is
tangled along with the full Makefile, it simply gets tacked onto the end of the
big Makefile  as an additional rule.   After 'preprocess.el' runs, and  the new
Makefile  is  extracted,  the  script  runs 'git'  to  update  the  repository,
including pushing the changes to Github.

Now, running ~make~ runs  the default rule from the main  Makefile, which is to
extract everything, then export to TEXI, INFO, HTML, and PDF forms.

It is assumed that an Emacs server is running, and that the $EDITOR environment
variable is set to use ~emacsclient~.

#+name:boot-template
#+header: :tangle Makefile
#+begin_src makefile
  boot:
	  $(EDITOR) -u --eval \
		  "(with-current-buffer (car (find-file-noselect \"./*.org\" nil nil t)) \
			  (goto-char (point-min)) \
			  (re-search-forward \"^#[+]name:preprocess.el$$\") \
			  (org-babel-tangle (quote (4))) \
			  (save-buffer) \
			  (kill-buffer))" \
	  --eval \
		  "(let ((rsrcdir \"resources\") \
			 (subdirs (list \"tools\" \"images\"))) \
		     (mkdir rsrcdir t) \
		     (dolist (subdir subdirs) (mkdir (concat rsrcdir \"/\" subdir) t)))"
	  ./resources/tools/preprocess.el
	  git add . && git commit -m "After running boot-template Makefile" && git push origin master
#+end_src

** Preprocess Env Vars
The environment variable DEV can be  in different locations and will be spelled
differently based  on how the  local machine is set  up.  For instance,  on one
system,  it will  be at  ~$HOME/Dev~  while in  another  system it  will be  at
~/usr/local/dev~.  However, the =:tangle= keyword  does not expand variables in
the form ~${DEV}~,  but rather requires absolute  paths, like ~/usr/local/dev~.
Therefore, this program works like a preprocessor for environment variables set
up  as part  of  =:tangle= lines,  changing them  to  their system  environment
variable values prior to tangling.  It lives in the ~resources/tools~
directory.

- *NOTE:  [2021-09-15 Wed  23:30]* The  assumption  that the  emacs program  is
  located   at  ~/opt/local/bin/~   is   incorrect.   Perhaps   it  should   be
  ~#!/usb/bin/env emacs~ instead.

#+name:preprocess.el
#+header: :mkdirp t
#+header: :tangle resources/tools/preprocess.el
#+header: :shebang "#!/usr/bin/env emacs -Q --script"
#+begin_src emacs-lisp
  (with-current-buffer (car (find-file-noselect "./*.org" nil nil t))
    (save-excursion
    (goto-char (point-min))
    (let ((re-search-str "\\(?::tangle\\|load-file \\(?:[\\]*\\)?[\"]\\)\s*\\(.*?/[dD]ev\\)/")
          (dev (getenv "DEV")))
      (while
              (re-search-forward re-search-str nil t)
              (replace-match dev t nil nil 1)))
    (save-buffer)
    (require 'org)
    (org-babel-tangle)))
#+end_src

** Samples
#+begin_comment
(cd "~/Dev/Emacs/MasteringEmacs/")
"/Users/pine/Dev/Emacs/MasteringEmacs/"

(defun add-bucket (org bucket)
  "Add a bucket keyword BUCKET to the org file ORG."
  (interactive "fFile: \nsBUCKET: ")
  (with-current-buffer (find-file-noselect org)
    (let* ((tree (org-element-parse-buffer))
	   (ins (car (org-element-map tree (quote section)
		 (lambda (s)
		   (org-element-map s (quote keyword)
		     (lambda (kw) (when (equal "MACRO" (org-element-property :key kw)) (1- (org-element-property :end kw))))
		     nil nil :keyword))
		 nil t nil nil))))
      (goto-char ins)
      (insert (format "#+bucket:%s\n" bucket))
      ())))

(add-bucket "MasteringEmacs.org" "pinecone-forest")
nil

(defun hl-region (raw-hl)
  "Obtain the begin and end positions for a headline."
  (with-current-buffer (find-file-noselect (getenv "SYNC_ORG_TEMPLATE"))
    (let* ((tree (get-parsed-tree))
	   (hl (car-safe (org-element-map tree 'headline
			   (lambda (hl) (when
					    (string= raw-hl
						     (org-element-property :raw-value hl))
					  (org-element-context)))
			   nil nil t))))
      (cons
       (org-element-property :begin hl)
       (org-element-property :end hl))
      )))

(hl-region "Build Tools")

(4888 . 29646)

(defun get-hl-with-prop (org-dir hl-prop)
  "Given a directory containing an Org template file and a custom_id property name, return the headline containing that custom_id, or nil if none."
  (progn
    (cd org-dir)
    (let ((org-buf (car-safe (find-file-noselect "*.org" nil nil t))))
      (if org-buf
	  (with-current-buffer org-buf
	    (let ((tree (org-element-parse-buffer)))
	      (org-element-map tree 'headline
		(lambda (hl)
		  (let ((cid (org-element-property :CUSTOM_ID hl)))
		    (when (string= hl-prop cid)
		      (and
		       (message (format "Found the headline %s containing property %s." (org-element-property :raw-value hl) hl-prop))
		       hl))))
		nil t)))
	(and
	 (message (format "The directory %s does not contain an Org file." org-dir))
	 nil)))))

(get-hl-with-prop "~/Dev/Templates/Org" "build-tools")

(headline (:raw-value "Build Tools" :begin 4888 :end 29646 :pre-blank 0 :contents-begin 4902 :contents-end 29645 :level 1 :priority nil :tags nil :todo-keyword nil :todo-type nil :post-blank 1 :footnote-section-p nil :archivedp nil :commentedp nil :post-affiliated 4888 :FROM-FILE "Template" :CUSTOM_ID "build-tools" :APPENDIX "t" :title "Build Tools"))









;;; Add a keyword named 'bucket' just after the version macro.
;;; This function should be run from within the directory containing the Org file.
(defun add-bucket (org-file s3-bucket)
  "Add the name of the associated AWS S3 bucket to an Org templated file."
  (with-current-buffer (find-file-noselect org-file)
    (goto-char (point-min))
    (let* ((tree (org-element-parse-buffer))
	   ;; find the beginning position of the first headline to act as a limit
	   (hl1 (org-element-map tree (quote headline) (lambda (hl) (org-element-property :begin hl)) nil t)))
      ;; Check for the presence of a bucket keyword before the first headline
      (unless (re-search-forward "^#\\+bucket:" hl1 t)
	;; If no bucket keyword is found, search for a keyword MACRO with the value 'version'
	(org-element-map tree (quote keyword)
	  (lambda (kw) (when (and (string= "MACRO" (org-element-property :key kw))
				  (string-match-p "version" (org-element-property :value kw)))
			 ;; return the end position of the MACRO; subtract an empty line if there is one
			 (goto-char (- (org-element-property :end kw) (org-element-property :post-blank kw)))
			 (insert "#+bucket:" s3-bucket)
			 (newline)
			 (basic-save-buffer)
			 (message (format "Added bucket %s" s3-bucket))))
	  nil t)))))

(add-bucket "MasteringEmacs.org" "pinecone-forest.com")
nil

"Added bucket pinecone-forest.com"









(keyword (:key "MACRO" :value "version Version 0.0.108" :begin 148 :end 181 :post-blank 1 :post-affiliated 148 ...))
("TITLE" "SUBTITLE" "AUTHOR" "DATE" "MACRO" "TEXINFO" "TEXINFO" "CINDEX" "CINDEX" "CINDEX" "CINDEX" "CINDEX" ...)







((keyword (:key "MACRO" :value "version Version 0.0.107" :begin 148 :end 181 :post-blank 1 :post-affiliated 148 ...)))
#+end_comment

* List of Programs
:PROPERTIES:
:appendix: t
:END:
#+texinfo:@listoffloats Listing

* List of Examples
:PROPERTIES:
:appendix: t
:END:
#+texinfo:@listoffloats Example

* List of Tables
:PROPERTIES:
:appendix: t
:END:
#+texinfo:@listoffloats Table

* Copying
:PROPERTIES:
:copying:  t
:END:

Copyright \copy 2020 by {{{author}}}

* Concept Index
:PROPERTIES:
:index: cp
:appendix: yes
:END:

* Program Index
:PROPERTIES:
:index: pg
:appendix: yes
:END:

* Function Index
:PROPERTIES:
:index: fn
:appendix: yes
:END:

* Variable Index
:PROPERTIES:
:index: vr
:appendix: yes
:END:


* Configuration							   :noexport:
#+startup:content

#+todo: SOMEDAY(s@) TODO(t@) INPROGRESS(i@) WAIT(w@) | CANCEL(c@) DONE(d!)

#+options: H:4 ':t

#+texinfo_class: info
#+texinfo_header:
#+texinfo_post_header:
#+texinfo_dir_category:<DIR CATEGORY>
#+texinfo_dir_title:<DIR TITLE>
#+texinfo_dir_desc:<DIR DESCRIPTION>
#+texinfo_printed_title:DeployHugoWebsite---Deploy a Hugo Website with Cloud Build and Firebase Pipeline


* Local Variables						   :noexport:

* Footnotes

[fn:1] https://console.firebase.google.com/

[fn:2] mistake? should be ~config.yaml~ ??

[fn:3]In the browser, add =index.text= to the end of the URL to see the source.

[fn:4]Markdown requires the standard Perl library module Digest::MD5.


# Local Variables:
# fill-column: 79
# indent-tabs-mode: t
# eval: (auto-fill-mode)
# time-stamp-pattern: "8/^\\#\\+date:%:y-%02m-%02d %02H:%02M$"
# End:
